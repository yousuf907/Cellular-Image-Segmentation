{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = 0.7)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "from itertools import chain\n",
    "from skimage.io import imread, imshow, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout\n",
    "from keras.layers.core import Lambda, RepeatVector, Reshape\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D, GlobalMaxPool2D\n",
    "from keras.layers.merge import concatenate, add\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm_notebook #, tnrange\n",
    "#from itertools import chain\n",
    "from skimage.io import imread, imshow #, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Model, load_model, save_model\n",
    "from keras.layers import Input,Dropout,BatchNormalization,Activation,Add\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.preprocessing.image import array_to_img, img_to_array, load_img#,save_img\n",
    "\n",
    "import time\n",
    "t_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some parameters\n",
    "im_width = 240\n",
    "im_height = 240\n",
    "border = 5\n",
    "path_train = \"C:/Users/yousu/Downloads/BlastsOnline\"\n",
    "\n",
    "# Get and resize train images and masks\n",
    "def get_data(path, train=True):\n",
    "    ids = next(os.walk(path + \"/imagesResized\"))[2]\n",
    "    ids2= next(os.walk(path + \"/NewLabels\"))[2]\n",
    "    X = np.zeros((len(ids), im_height, im_width, 1), dtype=np.float32)\n",
    "    if train:\n",
    "        y = np.zeros((len(ids2), im_height, im_width, 1), dtype=np.float32)\n",
    "    print('Getting and resizing images ... ')\n",
    "    for n, id_ in tqdm_notebook(enumerate(ids), total=len(ids)):\n",
    "        # Load images\n",
    "        img = load_img(path + '/imagesResized/' + id_, grayscale=True)\n",
    "        x_img = img_to_array(img)\n",
    "        x_img = resize(x_img, (im_width, im_height, 1), mode='constant', preserve_range=True)\n",
    "        X[n] = x_img / 255.0\n",
    "\n",
    "    for n, id_ in tqdm_notebook(enumerate(ids2), total=len(ids2)):\n",
    "        # Load masks\n",
    "        if train:\n",
    "            mask = img_to_array(load_img(path + '/NewLabels/' + id_, grayscale=True))\n",
    "            mask = resize(mask, (im_width, im_height, 1), mode='constant', preserve_range=True)\n",
    "            y[n] = mask / 255.0\n",
    "\n",
    "        # Save images\n",
    "        #X[n] = x_img / 255\n",
    "        #if train:\n",
    "            #y[n] = mask / 255\n",
    "    print('Done!')\n",
    "    if train:\n",
    "        return X, y\n",
    "    else:\n",
    "        return X\n",
    "    \n",
    "X, y = get_data(path_train, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and valid\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.25, random_state=2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentation_models import Unet\n",
    "from segmentation_models.backbones import get_preprocessing\n",
    "from segmentation_models.losses import bce_jaccard_loss\n",
    "from segmentation_models.losses import bce_dice_loss\n",
    "from segmentation_models.metrics import iou_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BatchActivate(x):\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def convolution_block(x, filters, size, strides=(1,1), padding='same', activation=True):\n",
    "    x = Conv2D(filters, size, strides=strides, padding=padding)(x)\n",
    "    if activation == True:\n",
    "        x = BatchActivate(x)\n",
    "    return x\n",
    "\n",
    "def residual_block(blockInput, num_filters=16, batch_activate = False):\n",
    "    x = BatchActivate(blockInput)\n",
    "    x = convolution_block(x, num_filters, (3,3))\n",
    "    x = convolution_block(x, num_filters, (3,3), activation=False)\n",
    "    x = Add()([x, blockInput])\n",
    "    if batch_activate:\n",
    "        x = BatchActivate(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "def build_model(input_layer, start_neurons, DropoutRatio = 0.5):\n",
    "    # 101 -> 50\n",
    "    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=None, padding=\"same\")(input_layer)\n",
    "    #conv1 = residual_block(conv1,start_neurons * 1)\n",
    "    conv1 = residual_block(conv1,start_neurons * 1, True)\n",
    "    pool1 = MaxPooling2D((2, 2))(conv1)\n",
    "    pool1 = Dropout(DropoutRatio/2)(pool1)\n",
    "\n",
    "    # 50 -> 25\n",
    "    conv2 = Conv2D(start_neurons * 2, (3, 3), activation=None, padding=\"same\")(pool1)\n",
    "    #conv2 = residual_block(conv2,start_neurons * 2)\n",
    "    conv2 = residual_block(conv2,start_neurons * 2, True)\n",
    "    pool2 = MaxPooling2D((2, 2))(conv2)\n",
    "    pool2 = Dropout(DropoutRatio)(pool2)\n",
    "\n",
    "    # 25 -> 12\n",
    "    conv3 = Conv2D(start_neurons * 4, (3, 3), activation=None, padding=\"same\")(pool2)\n",
    "    #conv3 = residual_block(conv3,start_neurons * 4)\n",
    "    conv3 = residual_block(conv3,start_neurons * 4, True)\n",
    "    pool3 = MaxPooling2D((2, 2))(conv3)\n",
    "    pool3 = Dropout(DropoutRatio)(pool3)\n",
    "\n",
    "    # 12 -> 6\n",
    "    conv4 = Conv2D(start_neurons * 8, (3, 3), activation=None, padding=\"same\")(pool3)\n",
    "    #conv4 = residual_block(conv4,start_neurons * 8)\n",
    "    conv4 = residual_block(conv4,start_neurons * 8, True)\n",
    "    pool4 = MaxPooling2D((2, 2))(conv4)\n",
    "    pool4 = Dropout(DropoutRatio)(pool4)\n",
    "\n",
    "    # Middle\n",
    "    convm = Conv2D(start_neurons * 16, (3, 3), activation=None, padding=\"same\")(pool4)\n",
    "    #convm = residual_block(convm,start_neurons * 16)\n",
    "    convm = residual_block(convm,start_neurons * 16, True)\n",
    "    \n",
    "    #Dilated Middle\n",
    "    #convm = bottleneck(pool4, filters_bottleneck=start_neurons * 16, mode='cascade', depth=4,\n",
    "               #kernel_size=(3, 3), activation='relu')\n",
    "    #convm = residual_block(convm,start_neurons * 16, True)\n",
    "    \n",
    "    # 6 -> 12\n",
    "    deconv4 = Conv2DTranspose(start_neurons * 8, (3, 3), strides=(2, 2), padding=\"same\")(convm)\n",
    "    uconv4 = concatenate([deconv4, conv4])\n",
    "    #uconv4 = BatchNormalization()(uconv4)\n",
    "    uconv4 = Dropout(DropoutRatio)(uconv4)\n",
    "    \n",
    "    uconv4 = Conv2D(start_neurons * 8, (3, 3), activation=None, padding=\"same\")(uconv4)\n",
    "    #uconv4 = residual_block(uconv4,start_neurons * 8)\n",
    "    uconv4 = residual_block(uconv4,start_neurons * 8, True)\n",
    "    \n",
    "    # 12 -> 25\n",
    "    deconv3 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding=\"same\")(uconv4)\n",
    "    #deconv3 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding=\"valid\")(uconv4)\n",
    "    uconv3 = concatenate([deconv3, conv3])\n",
    "    #uconv3 = BatchNormalization()(uconv3)\n",
    "    uconv3 = Dropout(DropoutRatio)(uconv3)\n",
    "    \n",
    "    uconv3 = Conv2D(start_neurons * 4, (3, 3), activation=None, padding=\"same\")(uconv3)\n",
    "    #uconv3 = residual_block(uconv3,start_neurons * 4)\n",
    "    uconv3 = residual_block(uconv3,start_neurons * 4, True)\n",
    "\n",
    "    # 25 -> 50\n",
    "    deconv2 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), padding=\"same\")(uconv3)\n",
    "    uconv2 = concatenate([deconv2, conv2])\n",
    "    #uconv2 = BatchNormalization()(uconv2)\n",
    "        \n",
    "    uconv2 = Dropout(DropoutRatio)(uconv2)\n",
    "    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=None, padding=\"same\")(uconv2)\n",
    "    #uconv2 = residual_block(uconv2,start_neurons * 2)\n",
    "    uconv2 = residual_block(uconv2,start_neurons * 2, True)\n",
    "    \n",
    "    # 50 -> 101\n",
    "    deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"same\")(uconv2)\n",
    "    #deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"valid\")(uconv2)\n",
    "    uconv1 = concatenate([deconv1, conv1])\n",
    "    #uconv1 = BatchNormalization()(uconv1)\n",
    "    \n",
    "    uconv1 = Dropout(DropoutRatio)(uconv1)\n",
    "    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=None, padding=\"same\")(uconv1)\n",
    "    #uconv1 = residual_block(uconv1,start_neurons * 1)\n",
    "    uconv1 = residual_block(uconv1,start_neurons * 1, True)\n",
    "    \n",
    "    #uconv1 = Dropout(DropoutRatio/2)(uconv1)\n",
    "    #output_layer = Conv2D(1, (1,1), padding=\"same\", activation=\"sigmoid\")(uconv1)\n",
    "    output_layer_noActi = Conv2D(1, (1,1), padding=\"same\", activation=None)(uconv1)\n",
    "    output_layer =  Activation('sigmoid')(output_layer_noActi)\n",
    "    \n",
    "    return output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "img_size_target = 240\n",
    "input_layer = Input((img_size_target, img_size_target, 1))\n",
    "output_layer = build_model(input_layer, 16,0.05)\n",
    "lr=.0001\n",
    "# del model\n",
    "model_resunet1 = Model(input_layer, output_layer)\n",
    "model_resunet1.compile(optimizer=Adam(lr), loss=bce_jaccard_loss, metrics=[iou_score])\n",
    "\n",
    "model_resunet1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model\n",
    "model_resunet2 = Model(input_layer, output_layer)\n",
    "model_resunet2.compile(optimizer=Adam(lr), loss=bce_dice_loss, metrics=[iou_score])\n",
    "\n",
    "model_resunet2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model\n",
    "model_resunet3 = Model(input_layer, output_layer)\n",
    "model_resunet3.compile(optimizer=Adam(lr), loss=\"binary_crossentropy\", metrics=[iou_score])\n",
    "\n",
    "model_resunet3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model\n",
    "model_resunet4 = Model(input_layer, output_layer)\n",
    "model_resunet4.compile(optimizer=Adam(lr), loss=\"mean_squared_error\", metrics=[iou_score])\n",
    "\n",
    "model_resunet4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resunet1.load_weights('model-ResUnet_aug.h5')\n",
    "model_resunet2.load_weights('model-ResUnet_bcedice.h5')\n",
    "model_resunet3.load_weights('model-ResUnet_bce.h5')\n",
    "model_resunet4.load_weights('model-ResUnet_mse.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "members = [model_resunet1,model_resunet2,model_resunet3,model_resunet4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predictions(members, weights, testX):\n",
    "# make predictions\n",
    "    yhats = [model.predict(testX) for model in members]\n",
    "    yhats = np.array(yhats)\n",
    "# sum across ensemble members\n",
    "    avg = np.tensordot(yhats, weights, axes=((0),(0)))\n",
    "# argmax across classes\n",
    "    result = (avg > 0.5).astype(np.float32)  #Threshold 0.5\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equal weights\n",
    "weights = [0.25,0.25,0.25,0.25]\n",
    "#weights = [0, 0.33333333, 0, 0.66666667]\n",
    "epred = ensemble_predictions(members,weights, X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample(X, y, preds, binary_preds, ix=None):\n",
    "    if ix is None:\n",
    "        ix = random.randint(0, len(X))\n",
    "\n",
    "    has_mask = y[ix].max() > 0\n",
    "\n",
    "    fig, ax = plt.subplots(1, 4, figsize=(20, 10))\n",
    "    ax[0].imshow(X[ix, ..., 0], cmap='gray')\n",
    "    if has_mask:\n",
    "        ax[0].contour(y[ix].squeeze(), colors='b', levels=[0.5])\n",
    "    ax[0].set_title('Embryo')\n",
    "\n",
    "    ax[1].imshow(y[ix].squeeze(), vmin=0, vmax=2)\n",
    "    ax[1].set_title('Embryo_GT')\n",
    "\n",
    "    ax[2].imshow(preds[ix].squeeze(), vmin=0, vmax=1)\n",
    "    if has_mask:\n",
    "        ax[2].contour(y[ix].squeeze(), colors='k', levels=[0.5])\n",
    "    ax[2].set_title('Embryo Predicted')\n",
    "    \n",
    "    ax[3].imshow(binary_preds[ix].squeeze(), vmin=0, vmax=1)\n",
    "    if has_mask:\n",
    "        ax[3].contour(y[ix].squeeze(), colors='r', levels=[1])\n",
    "    ax[3].set_title('Embryo Predicted binary');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if valid data looks all right\n",
    "plot_sample(X_valid, y_valid, epred, epred, ix=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(y_true, y_pred):\n",
    "    y_true = tf.convert_to_tensor(y_true, np.float32)\n",
    "    y_pred = tf.convert_to_tensor(y_pred, np.float32)\n",
    "    epsilon = 1e-15\n",
    "    #intersection = (y_pred * y_true).sum(dim=-2).sum(dim=-1)\n",
    "    #union = y_true.sum(dim=-2).sum(dim=-1) + y_pred.sum(dim=-2).sum(dim=-1)\n",
    "    y_true_f = tf.layers.flatten(y_true)\n",
    "    y_pred_f = tf.layers.flatten(y_pred)\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "    union = tf.reduce_sum(y_true_f)+tf.reduce_sum(y_pred_f)\n",
    "\n",
    "    return (tf.reduce_mean(intersection + epsilon)/ (union - intersection + epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ensemble(members, weights, testX, testy):\n",
    "# make prediction\n",
    "     yhat = ensemble_predictions(members, weights, testX)\n",
    "# calculate accuracy\n",
    "     with tf.Session() as sess:\n",
    "          return sess.run(iou_score(testy, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluate_ensemble(members,weights,X_valid,y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "def normalize(weights):\n",
    "    # calculate l1 vector norm\n",
    "    result = norm(weights, 1)\n",
    "    # check for a vector of all zeros\n",
    "    if result == 0.0:\n",
    "        return weights\n",
    "    # return normalized vector (unit norm)\n",
    "    return weights / result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "# grid search weights\n",
    "#def grid_search(members):\n",
    "    # define weights to consider\n",
    "w = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "best_weights = []\n",
    "#c=0\n",
    "# iterate all possible combinations (cartesian product)\n",
    "for weights in product(w, repeat=4):\n",
    "    # skip if all weights are equal\n",
    "    #c=c+1\n",
    "    if len(set(weights)) == 1:\n",
    "        continue\n",
    "    # hack, normalize weight vector\n",
    "    weights = normalize(weights)\n",
    "    #if any((weights == x).all() for x in best_weights) ==True:\n",
    "        #continue\n",
    "    # evaluate weights\n",
    "    #score = evaluate_ensemble(members, weights, testX, testy)\n",
    "    #if score > best_score:\n",
    "    #best_weights = weights\n",
    "        #print('>%s %.4f' % (best_weights, best_score))\n",
    "    best_weights.append(weights)\n",
    "    #print(c)\n",
    "print(len(best_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_after_point(x):\n",
    "    s = str(x)\n",
    "    if not '.' in s:\n",
    "        return 0\n",
    "    return len(s) - s.index('.') - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_weights[9]==best_weights[:9]\n",
    "D=[]\n",
    "for i in range (len(best_weights)):\n",
    "    if any((best_weights[i-1] == x).all() for x in best_weights[:i-1]) ==False:\n",
    "        if num_after_point(best_weights[i-1][0])<2:\n",
    "            D.append(best_weights[i-1])\n",
    "print(len(D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(members, testX, testy):\n",
    "    #x=0\n",
    "    best_score, best_weights = 0.0, None\n",
    "    for j in range (100):\n",
    "        #best_score, best_weights = 0.0, None\n",
    "        score = evaluate_ensemble(members, D[j-1], X_valid, y_valid)\n",
    "        if score > best_score:\n",
    "            best_score, best_weights = score, D[j-1]\n",
    "            print('>%s %.5f' % (best_weights, best_score))\n",
    "            #print(x)\n",
    "    return list(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = grid_search(members,X_valid,y_valid)\n",
    "score = evaluate_ensemble(members, weights, X_valid, y_valid)\n",
    "print('Grid Search Weights: %s, Score: %.4f' % (weights, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "# compile the model\n",
    "#model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc',f1_m,precision_m, recall_m])\n",
    "\n",
    "# fit the model\n",
    "#history = model.fit(Xtrain, ytrain, validation_split=0.3, epochs=10, verbose=0)\n",
    "\n",
    "# evaluate the model\n",
    "y_valid = tf.convert_to_tensor(y_valid, np.float32)\n",
    "epred = tf.convert_to_tensor(epred, np.float32)\n",
    "#preds_val = tf.convert_to_tensor(preds_val, np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(sess.run(recall_m(y_valid,epred)))\n",
    "    print(sess.run(precision_m(y_valid,epred)))\n",
    "    print(sess.run(f1_m(y_valid,epred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
